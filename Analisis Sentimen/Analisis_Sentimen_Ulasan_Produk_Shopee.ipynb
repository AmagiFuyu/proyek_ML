{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Download Library Bahasa Indonesia"
      ],
      "metadata": {
        "id": "nxPgFEhztHOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSjf2Ycdts0v",
        "outputId": "fc8648ca-eb3f-4a03-c09e-f1556239f540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-dK-hzutNUH",
        "outputId": "807025e4-4589-4c89-f04b-1597f7f8e883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.11/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ],
      "metadata": {
        "id": "SfvuSLbtq6I4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpyffUW4qw6T",
        "outputId": "94ac97c2-2a31-4a64-a126-47a7c01b426c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# Import Library\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import nltk\n",
        "import json\n",
        "import random\n",
        "\n",
        "#Download NLTK Stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping Data"
      ],
      "metadata": {
        "id": "0rKEKmtarX3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scraping Data Playsotre\n",
        "def scrape_playstore_reviews(app_id, num_reviews=3000):\n",
        "    reviews = []\n",
        "    for page in range(1, num_reviews // 40 + 2):\n",
        "        url = f\"https://play.google.com/store/getreviews?authuser=0&reviewType=0&pageNum={page}&id={app_id}&reviewSortOrder=0&xhr=1\"\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0',\n",
        "            'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8'\n",
        "        }\n",
        "        data = f'reviewType=0&pageNum={page}&id={app_id}&reviewSortOrder=0&xhr=1'\n",
        "        response = requests.post(url, headers=headers, data=data)\n",
        "        try:\n",
        "            content = json.loads(response.text[6:])[0][2]\n",
        "            soup = BeautifulSoup(content, 'html.parser')\n",
        "            for div in soup.find_all('div', class_='review-body'):\n",
        "                text = div.text.strip()\n",
        "                if text:\n",
        "                    reviews.append(text)\n",
        "        except Exception:\n",
        "            continue\n",
        "        time.sleep(0.5)\n",
        "        if len(reviews) >= num_reviews:\n",
        "            break\n",
        "    return pd.DataFrame(reviews[:num_reviews], columns=['review'])\n",
        "\n"
      ],
      "metadata": {
        "id": "zQWzDEn6sER_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Text"
      ],
      "metadata": {
        "id": "nOlXQnu9sopX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPROCESSING TEXT\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    tokens = text.split()\n",
        "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)\n"
      ],
      "metadata": {
        "id": "Tp1s7m5UsoZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction & Data Split"
      ],
      "metadata": {
        "id": "sgYfdjdpuryI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggunakan Dataset Ulasan Shopee Shopee\n",
        "app_id = \"com.shopee.id\"\n",
        "df = scrape_playstore_reviews(app_id)\n",
        "\n",
        "# Tambahan data kompleks manual\n",
        "additional_reviews = [\n",
        "    \"Aplikasi ini sangat membantu, tapi kadang lemot kalau sinyal buruk. Overall oke lah.\",\n",
        "    \"Belanja pertama lancar, yang kedua barangnya lama banget dikirim. Kecewa sih.\",\n",
        "    \"UI/UX sudah membaik dari versi sebelumnya, tapi sistem pencarian masih tidak akurat.\",\n",
        "    \"Kenapa tiba-tiba aplikasi force close terus? Padahal sebelumnya lancar.\",\n",
        "    \"Pengalaman belanja sangat menyenangkan, pengiriman cepat, CS responsif. Good job!\",\n",
        "    \"Lumayan sih, kadang ada bug tapi sering update juga.\",\n",
        "    \"Saya sudah dua kali beli di sini dan selalu memuaskan. Penjual responsif, pengiriman cepat.\",\n",
        "    \"Setelah update terbaru, aplikasi sering ngelag. Harap segera diperbaiki.\",\n",
        "    \"Barangnya tidak sesuai deskripsi, sangat mengecewakan dan CS tidak membantu.\",\n",
        "    \"Fitur promo sering error saat checkout, padahal sinyal bagus dan aplikasi sudah diupdate.\",\n",
        "    \"Packing rapi, barang aman sampai tujuan. Terima kasih Shopee!\",\n",
        "    \"Cukup puas, hanya saja notifikasi suka telat muncul. Mohon ditingkatkan.\",\n",
        "    \"Awalnya lancar, tapi sekarang sering keluar sendiri dari aplikasi.\"\n",
        "]\n",
        "df_complex = pd.DataFrame(additional_reviews, columns=['review'])\n",
        "df = pd.concat([df, df_complex], ignore_index=True)\n",
        "\n",
        "positive_keywords = [\"bagus\", \"mantap\", \"cepat\", \"puas\", \"keren\", \"terbaik\", \"menyenangkan\", \"lancar\", \"responsif\", \"oke\"]\n",
        "negative_keywords = [\"jelek\", \"lemot\", \"buruk\", \"error\", \"gagal\", \"parah\", \"kecewa\", \"bug\", \"force close\"]\n",
        "\n",
        "def label_sentiment(text):\n",
        "    text = text.lower()\n",
        "    if any(word in text for word in positive_keywords):\n",
        "        return \"positif\"\n",
        "    elif any(word in text for word in negative_keywords):\n",
        "        return \"negatif\"\n",
        "    else:\n",
        "        return \"netral\"\n",
        "\n",
        "df[\"label\"] = df[\"review\"].apply(label_sentiment)\n",
        "df = df[df['label'].isin(['positif', 'netral', 'negatif'])]\n",
        "df[\"clean_review\"] = df[\"review\"].apply(preprocess_text)\n",
        "\n",
        "X = df[\"clean_review\"]\n",
        "y = df[\"label\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "yHMhtdJkuyk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "Aof4nvFyvG6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Melatih Model\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "1NEHPxydvKRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1: Logistic Regression"
      ],
      "metadata": {
        "id": "Rj6LA_SNvVvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logisitic Regression\n",
        "lr = LogisticRegression(max_iter=300)\n",
        "lr.fit(X_train_tfidf, y_train)\n",
        "y_pred_lr = lr.predict(X_test_tfidf)\n",
        "acc_lr = accuracy_score(y_test, y_pred_lr) * 100"
      ],
      "metadata": {
        "id": "CUj68flfvjIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2: SVM"
      ],
      "metadata": {
        "id": "qVb0Yu-DvaPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM\n",
        "svm = SVC()\n",
        "svm.fit(X_train_tfidf, y_train)\n",
        "y_pred_svm = svm.predict(X_test_tfidf)\n",
        "acc_svm = accuracy_score(y_test, y_pred_svm) * 100"
      ],
      "metadata": {
        "id": "XAtG3T3JvtA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3: Random Forest"
      ],
      "metadata": {
        "id": "wel72_t2vdc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train_tfidf, y_train)\n",
        "y_pred_rf = rf.predict(X_test_tfidf)\n",
        "acc_rf = accuracy_score(y_test, y_pred_rf) * 100"
      ],
      "metadata": {
        "id": "8n9fzpwdvuGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluasi Tiap Model"
      ],
      "metadata": {
        "id": "BSeYwKWEv_DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nAkurasi Logistic Regression: {:.2f}%\".format(acc_lr))\n",
        "print(\"\\nAkurasi SVM: {:.2f}%\".format(acc_svm))\n",
        "print(\"\\nAkurasi Random Forest: {:.2f}%\".format(acc_rf))\n",
        "\n",
        "print(\"\\nClassification Report - Logistic Regression:\")\n",
        "print(classification_report(y_test, y_pred_lr))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWGKou9kv8BA",
        "outputId": "ef5efcda-a59a-4885-d34a-ad0843c695c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Akurasi Logistic Regression: 100.00%\n",
            "\n",
            "Akurasi SVM: 100.00%\n",
            "\n",
            "Akurasi Random Forest: 100.00%\n",
            "\n",
            "Classification Report - Logistic Regression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     positif       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           1.00         3\n",
            "   macro avg       1.00      1.00      1.00         3\n",
            "weighted avg       1.00      1.00      1.00         3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simpan dataet"
      ],
      "metadata": {
        "id": "f1OR_eZ22N7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan dataset ke file CSV\n",
        "df.to_csv(\"dataset_ulasan_shopee.csv\", index=False)\n",
        "print(\"Dataset berhasil disimpan ke 'dataset_ulasan_shopee.csv'\")"
      ],
      "metadata": {
        "id": "AB6GCiEC2Qy-",
        "outputId": "818e2c2e-4c5f-4fed-8576-68de0f5fa22b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset berhasil disimpan ke 'dataset_ulasan_shopee.csv'\n"
          ]
        }
      ]
    }
  ]
}